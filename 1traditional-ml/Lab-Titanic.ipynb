{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic生存预测\n",
    "\n",
    "\n",
    "### 问题定义\n",
    "\n",
    "1. 问题定义\n",
    "\n",
    "- 核心问题：根据泰坦尼克号乘客的信息（如年龄、性别、舱位等），预测其是否能在灾难中生存。  \n",
    "\n",
    "- 背景：泰坦尼克号事故中，乘客生存率受多种因素影响（如“妇女儿童优先”的救援策略）。  \n",
    " \n",
    "- 业务目标：构建模型预测生存结果，探索影响生存的关键因素。  \n",
    "\n",
    "\n",
    "2. 是否适合机器学习？  \n",
    "\n",
    "适用性：\n",
    "\n",
    "- 监督学习任务：数据包含明确的标签（Survived: 0/1），适合分类算法（如逻辑回归、随机森林）。\n",
    "\n",
    "- 结构化数据：数据集为表格形式（乘客信息），包含数值型和类别型特征。\n",
    "\n",
    "- 明确模式：历史数据中隐藏着生存规律（如女性生存率更高）。\n",
    "\n",
    "\n",
    "潜在挑战：\n",
    "\n",
    "- 数据量较小（约900条训练样本），需防止过拟合。\n",
    "\n",
    "- 存在缺失值（如Age）、需特征工程（如从Name提取头衔）。\n",
    "\n",
    "3. 问题陈述   \n",
    "\n",
    "输入：乘客的特征（如Pclass, Sex, Age, Fare, Embarked等）。  \n",
    "输出：二分类结果（0=未生存，1=生存）。  \n",
    "评估指标：通常使用准确率（Accuracy），但数据不平衡时需考虑精确率/召回率。  \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 数据准确"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 1: Data Collection and Loading    ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# First, we collect the dataset from various sources (CSV, database, API, etc.).\n",
    "# Make sure the data is in a consistent format and loaded into memory properly.\n",
    "# Ensure that the dataset contains all the necessary features and labels.\n",
    "# Check for any corruption or inconsistencies in the data.\n",
    "titanic_ds = sns.load_dataset(\"titanic\")\n",
    "print(titanic_ds.shape)\n",
    "print(titanic_ds.count())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 数据清洗"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 2: Data Cleaning and Preparation  ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# The next step is to clean the data. We will check for missing or null values.\n",
    "# Missing data will be either filled with an appropriate value or dropped.\n",
    "# Ensure that the features are correctly formatted (e.g., numeric, categorical).\n",
    "# If necessary, encode categorical variables into numerical values.\n",
    "# Also, handle any outliers or erroneous data that could affect model performance.\n",
    "\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'class', 'who', 'adult_male', 'alone']\n",
    "target = 'survived'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = titanic_ds[features]\n",
    "y = titanic_ds[target]\n",
    "print(y.value_counts())\n",
    "print(titanic_ds['deck'].value_counts()) # 只有两百个有数据"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征工程"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 3: Feature Engineering             ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# In this step, we will extract and create new features from the raw data.\n",
    "# This could involve creating interaction terms, aggregating data, or transforming features.\n",
    "# Select relevant features that are likely to help the model learn better.\n",
    "# You may also scale or normalize the features to ensure they are on a similar range."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 数据集划分"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 4: Train-Test Split               ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# Now, we will split the dataset into training and testing sets.\n",
    "# Typically, 80% of the data is used for training and 20% for testing.\n",
    "# Ensure that the split is random but that the classes are well-represented in both sets.\n",
    "# This helps to avoid bias and ensures a fair evaluation of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "numerical_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # 使用中位数补充缺失值\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # 用最频繁的值补充缺失值\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # 转成n-1个不重复0-1特征\n",
    "])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模型选择"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 5: Model Selection               ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# Select an appropriate machine learning model for the task at hand.\n",
    "# This could be a classification algorithm (e.g., Random Forest, SVM) or regression model.\n",
    "# Consider the size of the dataset and the problem's complexity when choosing the model.\n",
    "# Tune hyperparameters to find the optimal configuration for the model.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模型训练"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 6: Model Training                ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# Now, we will train the selected model using the training data.\n",
    "# Feed the features and labels to the model, and let it learn the patterns in the data.\n",
    "# Keep track of the training progress, and ensure that the model is fitting well to the data.\n",
    "# If necessary, adjust hyperparameters or use techniques like cross-validation to improve training.\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "model = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=2)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模型评估"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 7: Model Evaluation              ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# Once the model is trained, it's time to evaluate its performance.\n",
    "# We will use the testing set to make predictions and compare them to the actual labels.\n",
    "# Calculate various metrics like accuracy, precision, recall, F1-score, etc.\n",
    "# Make sure that the model generalizes well to new, unseen data and is not overfitting.\n",
    "y_pred = model. predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 超参数调优"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ╔════════════════════════════════════════╗\n",
    "# ║ Step 8: Hyperparameter Tuning         ║\n",
    "# ╚════════════════════════════════════════╝\n",
    "# After evaluating the model, you may notice that it can be improved.\n",
    "# Use techniques like Grid Search or Random Search to tune the hyperparameters.\n",
    "# Try different values for parameters such as learning rate, number of trees, etc.\n",
    "# Evaluate the model again after tuning to see if performance improves.\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Titanic Classification Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "feature_importances = model.best_estimator_['classifier'].feature_importances_\n",
    "\n",
    "# Combine the numerical and one-hot encoded categorical feature names\n",
    "feature_names = numerical_features + list(model.best_estimator_['preprocessor']\n",
    "                                        .named_transformers_['cat']\n",
    "                                        .named_steps['onehot']\n",
    "                                        .get_feature_names_out(categorical_features))\n",
    "importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                              'Importance': feature_importances\n",
    "                             }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Most Important Features in predicting whether a passenger survived')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Print test score\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(f\"\\nTest set accuracy: {test_score:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
