{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/jimmy-pink/colab-playground/blob/main/kaggle/Sentiment140.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ⚜️ 《Sentiment140》\n",
    "[kaggle - sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)  \n",
    "[tensorflow-dataset-sentiment140 (variation)](https://www.tensorflow.org/datasets/catalog/sentiment140)"
   ],
   "metadata": {
    "id": "Cmbes9S-gmPq"
   }
  },
  {
   "metadata": {
    "id": "s8NO6XlSmEsx"
   },
   "cell_type": "markdown",
   "source": [
    "### 问题分析\n",
    "\n",
    "- 任务目标  \n",
    "  - 输入：一条推文文本（如 \"I love this movie! #happy\"）\n",
    "  - 输出：二分类情感标签（0=负面, 1=正面）\n",
    "\n",
    "- 数据特点  \n",
    "  - 数据量较大（160万条推文），适合练习大规模文本处理。\n",
    "  - 推文包含噪音（如表情符号、话题标签、@用户名等），需清洗。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorflow"
   ],
   "metadata": {
    "id": "gv22oHDb-fhL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:54:10.349709Z",
     "start_time": "2025-05-04T15:54:05.544263Z"
    },
    "id": "sNIpAoEBmEsx"
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_LENGTH = 32\n",
    "TRAINING_SPLIT = 0.9\n",
    "BATCH_SIZE = 128"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "_8lAkG_cmEsy"
   },
   "cell_type": "markdown",
   "source": [
    "### 数据准备\n",
    "\n",
    "- 目标变量0和4 优化为 0和1\n",
    "- text文本数据清洗"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.384697Z",
     "start_time": "2025-05-04T15:54:10.361214Z"
    },
    "id": "RyVWNAKkmEsy"
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load('sentiment140', split='train', shuffle_files=True)\n",
    "\n",
    "# 将 tf.data.Dataset 转换为 Pandas DataFrame\n",
    "def dataset_to_dataframe(dataset):\n",
    "    # 将 dataset 转换为 Pandas DataFrame\n",
    "    df = pd.DataFrame(list(dataset.as_numpy_iterator()))\n",
    "    return df\n",
    "\n",
    "# 转换\n",
    "df = dataset_to_dataframe(ds)\n",
    "\n",
    "# 查看前几行数据\n",
    "df.head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 数据清洗"
   ],
   "metadata": {
    "id": "qUuo2UbN_njn"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.700087Z",
     "start_time": "2025-05-04T15:55:45.685222Z"
    },
    "id": "7l2VERIRmEsy"
   },
   "cell_type": "code",
   "source": [
    "df.polarity.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.758854Z",
     "start_time": "2025-05-04T15:55:45.741622Z"
    },
    "id": "2WigDzVNmEsy"
   },
   "cell_type": "code",
   "source": [
    "# df['polarity'] = df.polarity.apply(lambda x: 0 if x == 0 else 1).to_numpy()\n",
    "df[\"polarity\"] = df[\"polarity\"].replace(4, 1)\n",
    "df.polarity.value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:49.444343Z",
     "start_time": "2025-05-04T15:55:45.804598Z"
    },
    "id": "CP3f2VrvmEsy"
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "texts = df[\"text\"].values\n",
    "\n",
    "# 文本清洗函数\n",
    "def clean_text(text):\n",
    "    # 确保文本是字符串类型\n",
    "    text = text.decode('utf-8') if isinstance(text, bytes) else text\n",
    "    # 移除@用户名\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # 移除URL\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # 只保留字母和空格\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # 转为小写\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# 应用清洗\n",
    "cleaned_texts = [clean_text(text) for text in texts]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:51.321232Z",
     "start_time": "2025-05-04T15:55:49.511497Z"
    },
    "id": "YmTo8KZQmEsz"
   },
   "cell_type": "code",
   "source": [
    "sentences = np.array(cleaned_texts)\n",
    "labels = df['polarity'].to_numpy()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences, labels))\n",
    "\n",
    "for i, (sentence, label) in enumerate(dataset.take(2)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Sentence: {sentence.numpy().decode('utf-8')}\")\n",
    "    print(f\"Label: {label.numpy()}\")\n",
    "    print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:51.366097Z",
     "start_time": "2025-05-04T15:55:51.353813Z"
    },
    "id": "B2pDk9BomEsz"
   },
   "cell_type": "code",
   "source": [
    "# 设置训练集比例\n",
    "# 计算训练集大小\n",
    "total_size = len(sentences)\n",
    "train_size = int(total_size * TRAINING_SPLIT)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# 打乱数据集（使用 buffer_size 作为参数，推荐设置为数据集大小）\n",
    "dataset = dataset.shuffle(buffer_size=total_size, reshuffle_each_iteration=True)\n",
    "\n",
    "# 将数据集拆分为训练集和验证集\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "train_dataset = (train_dataset\n",
    "                   .shuffle(10000)\n",
    "                   .cache()\n",
    "                   .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                   .batch(BATCH_SIZE)\n",
    "                   )\n",
    "validation_dataset = (validation_dataset\n",
    "                  .cache()\n",
    "                  .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                  .batch(BATCH_SIZE)\n",
    "                  )\n",
    "\n",
    "print(f\"There are {len(train_dataset)} batches for a total of {BATCH_SIZE*len(train_dataset)} elements for training.\\n\")\n",
    "print(f\"There are {len(validation_dataset)} batches for a total of {BATCH_SIZE*len(validation_dataset)} elements for validation.\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "s0MCeFzSmEsz"
   },
   "cell_type": "markdown",
   "source": [
    "### NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 矢量化\n",
    "- 目标： 将文本数据转化为数值表示\n",
    "\n",
    "1. fit_vectorizer 函数\n",
    "- **TextVectorization** 是一个文本预处理层，它负责将文本转换为数字表示。具体来说，TextVectorization 会将每个单词映射为一个数字索引。\n",
    "    \n",
    "- **output_sequence_length=MAX_LENGTH**：输出的每个文本序列将被填充或截断到 MAX_LENGTH 长度。如果文本长度超过该值，则会被截断；如果文本长度不足，则会填充零。\n",
    "    \n",
    "- **standardize='lower_and_strip_punctuation'**：这个参数指定了对文本进行标准化的方式，包括将文本转换为小写字母并去除标点符号。\n",
    "    \n",
    "- **dataset.map(lambda x, y: x)**：这一步从 dataset 中提取文本部分（假设 dataset 中每个元素是 (text, label) 的元组），并创建一个新的数据集 full_tokens，只包含文本。\n",
    "    \n",
    "- **vectorizer.adapt(full_tokens)**：这个方法会遍历 full_tokens 数据集中的文本数据，构建一个词汇表，并将文本数据中的每个词映射到一个索引。adapt 的过程是学习数据集中所有单词的索引表示。\n"
   ],
   "metadata": {
    "id": "ToYrLAZ88vy_"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:06:06.819660Z",
     "start_time": "2025-05-04T16:06:06.816225Z"
    },
    "id": "15zI0zJAmEsz"
   },
   "cell_type": "code",
   "source": [
    "def fit_vectorizer(dataset):\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        # max_tokens=10000, # 生成的矢量化词库的最大词数\n",
    "        output_sequence_length=MAX_LENGTH,\n",
    "        standardize='lower_and_strip_punctuation'\n",
    "    )\n",
    "    full_tokens = dataset.map(lambda x, y: x)\n",
    "    vectorizer.adapt(full_tokens)\n",
    "    return vectorizer"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. vectorizer 的使用\n",
    "- 通过调用 fit_vectorizer(train_dataset) 来根据训练数据集 train_dataset 生成一个矢量化器 vectorizer。\n",
    "    \n",
    "- vectorizer.vocabulary_size() 返回构建的词汇表的大小，即数据集中有多少个独特的词被映射到了一个索引。\n",
    "\n",
    "\n",
    "- 对训练集和验证集的数据进行矢量化。map(lambda x, y: (vectorizer(x), y)) 会将每个文本 x 转换为其对应的词向量索引，同时保持标签 y 不变。\n",
    "    \n",
    "- train_dataset_vectorized 和 validation_dataset_vectorized 就是矢量化后的数据集，其中 x 已经是整数索引的表示，准备输入到模型中进行训练或验证。"
   ],
   "metadata": {
    "id": "V07YG9g69mNK"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:27:12.564431Z",
     "start_time": "2025-05-04T16:06:09.085693Z"
    },
    "id": "gZKmhqEfmEsz"
   },
   "cell_type": "code",
   "source": [
    "# Adapt the vectorizer to the training sentences\n",
    "vectorizer = fit_vectorizer(train_dataset)\n",
    "# Check size of vocabulary\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "train_dataset_vectorized = train_dataset.map(lambda x,y: (vectorizer(x), y))\n",
    "validation_dataset_vectorized = validation_dataset.map(lambda x,y: (vectorizer(x), y))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 词语嵌入\n",
    "- 词语嵌入是 神经网络模型 训练后得到的，每个 token（子词）对应一个固定维度的向量（如 100 维或 300 维），这些向量用于表示 token 的语义或上下文信息。\n",
    "\n",
    "- 数据集词汇表 vectorizer.get_vocabulary()\n",
    "\n",
    "\n",
    "> 利用预训练的 GloVe（Global Vectors for Word Representation）词向量，将 vectorizer 提供的词汇表（tokens）映射到相应的 GloVe 词向量，并构建一个嵌入矩阵（embedding matrix）。\n",
    "\n",
    "\n",
    "1. **下载 GloVe 词向量**：\n",
    "    \n",
    "    - 代码首先检查 glove.6B.100d.txt 文件是否存在，如果不存在，则使用 wget 命令从 Hugging Face 下载。\n",
    "        \n",
    "    \n",
    "2. **加载 GloVe 词向量**：\n",
    "    \n",
    "    - 接下来，它会读取 glove.6B.100d.txt 文件，并将每一行分解成单词和对应的 100 维词向量。\n",
    "        \n",
    "    - glove_embeddings 是一个字典，其中存储了每个单词及其对应的 100 维词向量。\n",
    "        \n",
    "    \n",
    "3. **构建词汇表索引**：\n",
    "    \n",
    "    - word_index 是 vectorizer.get_vocabulary() 的词汇表的映射，get_vocabulary() 通常会返回按词频排序的所有单词。\n",
    "        \n",
    "    - word_index 是一个字典，它将每个单词映射到词汇表中的索引。\n",
    "        \n",
    "    \n",
    "4. **构建嵌入矩阵**：\n",
    "    \n",
    "    - embeddings_matrix 是一个大小为 (vocab_size, EMBEDDING_DIM) 的矩阵，用来存储所有词汇表中单词的词向量。vocab_size 是词汇表的大小，而 EMBEDDING_DIM 是 GloVe 词向量的维度（在此为 100）。\n",
    "        \n",
    "    - 然后，代码遍历 word_index 中的每个单词，如果该单词在 GloVe 的词向量字典中存在（即 glove_embeddings.get(word) 不为 None），就将该单词的词向量保存到 embeddings_matrix 中。\n",
    "\n",
    "得到结果：\n",
    "\n",
    "```python\n",
    "word_index = {'cat': 0, 'dog': 1, 'fish': 2}\n",
    "embeddings_matrix = [\n",
    "    [0.1, 0.2, ..., 0.99],  # cat 的 100维词向量\n",
    "    [0.3, 0.4, ..., 0.88],  # dog 的 100维词向量\n",
    "    [0.2, 0.1, ..., 0.75]   # fish 的 100维词向量\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "id": "6h23ztBK75CZ"
   }
  },
  {
   "metadata": {
    "id": "CLkddDhpmEsz"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# glove.6B.100d.txt， 包含了 400,000 个单词（或子词）及其对应的 100维 的词向量。\n",
    "glove_file = './data/glove.6B.100d.txt'\n",
    "# 确认文件存在\n",
    "if os.path.exists(glove_file):\n",
    "    print(f\"File found: {glove_file}\")\n",
    "else:\n",
    "    !wget https://huggingface.co/arjahojnik/LSTM-sentiment-model/resolve/af8ba3a2939dd573a5d9384efecbe7f04137860b/glove.6B.100d.txt -P ./data/\n",
    "\n",
    "\n",
    "# Initialize an empty embeddings index dictionary\n",
    "glove_embeddings = {}\n",
    "\n",
    "# Read file and fill glove_embeddings with its contents\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = coefs"
   ]
  },
  {
   "metadata": {
    "id": "UkMT9Kl-mEsz"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "word_index = {x:i for i,x in enumerate(vectorizer.get_vocabulary())}"
   ]
  },
  {
   "metadata": {
    "id": "CN03IavumEsz"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "# Iterate all of the words in the vocabulary and if the vector representation for\n",
    "# each word exists within GloVe's representations, save it in the embeddings_matrix array\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "metadata": {
    "id": "MWXXVB1LmEsz"
   },
   "cell_type": "markdown",
   "source": [
    "### 建模"
   ]
  },
  {
   "metadata": {
    "id": "L0MO13GlmEs0"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "from keras import regularizers\n",
    "def create_model(vocab_size, pretrained_embeddings):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(None,)),\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            weights=[pretrained_embeddings],\n",
    "            trainable=True  # 允许微调词向量\n",
    "        ),\n",
    "        tf.keras.layers.SpatialDropout1D(0.2),\n",
    "        tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            32,\n",
    "            dropout=0.4,\n",
    "            recurrent_dropout=0.4,\n",
    "            return_sequences=True\n",
    "        )),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(0.02)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # 使用学习率衰减\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps, decay_rate\n",
    "    )\n",
    "\n",
    "    # 编译模型\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "metadata": {
    "id": "cI8UzGxqmEs0"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = create_model(vocab_size, embeddings_matrix)\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy') >= 0.95 or logs.get('val_loss') < 0.35 ):\n",
    "            self.model.stop_training = True\n",
    "callbacks = myCallback()\n",
    "\n",
    "history = model.fit(\n",
    "\ttrain_dataset_vectorized,\n",
    "\tepochs=20,\n",
    "\tvalidation_data=validation_dataset_vectorized,\n",
    "\tcallbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "metadata": {
    "id": "MJRZYEkqmEs0"
   },
   "cell_type": "markdown",
   "source": [
    "### 评估"
   ]
  },
  {
   "metadata": {
    "id": "pQbaJiW-mEs0"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle('Training and validation performance')\n",
    "\n",
    "for i, (data, label) in enumerate(zip([(acc, val_acc), (loss, val_loss)], [\"Accuracy\", \"Loss\"])):\n",
    "    ax[i].plot(epochs, data[0], 'r', label=\"Training \" + label)\n",
    "    ax[i].plot(epochs, data[1], 'b', label=\"Validation \" + label)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('epochs')"
   ]
  }
 ]
}
