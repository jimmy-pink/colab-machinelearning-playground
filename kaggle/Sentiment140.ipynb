{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyPIGrEfUyuHav6ixaTff4e8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ⚜️ 《Sentiment140》\n",
    "[kaggle - sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)  \n",
    "[tensorflow-dataset-sentiment140 (variation)](https://www.tensorflow.org/datasets/catalog/sentiment140)"
   ],
   "metadata": {
    "id": "Cmbes9S-gmPq"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 问题分析\n",
    "\n",
    "- 任务目标  \n",
    "  - 输入：一条推文文本（如 \"I love this movie! #happy\"）\n",
    "  - 输出：二分类情感标签（0=负面, 1=正面）\n",
    "\n",
    "- 数据特点  \n",
    "  - 数据量较大（160万条推文），适合练习大规模文本处理。\n",
    "  - 推文包含噪音（如表情符号、话题标签、@用户名等），需清洗。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:54:10.349709Z",
     "start_time": "2025-05-04T15:54:05.544263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_LENGTH = 32\n",
    "TRAINING_SPLIT = 0.9\n",
    "BATCH_SIZE = 128"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 数据加载与预处理\n",
    "\n",
    "- 目标变量0和4 优化为 0和1\n",
    "- text文本数据清洗"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.384697Z",
     "start_time": "2025-05-04T15:54:10.361214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load('sentiment140', split='train', shuffle_files=True)\n",
    "\n",
    "# 将 tf.data.Dataset 转换为 Pandas DataFrame\n",
    "def dataset_to_dataframe(dataset):\n",
    "    # 将 dataset 转换为 Pandas DataFrame\n",
    "    df = pd.DataFrame(list(dataset.as_numpy_iterator()))\n",
    "    return df\n",
    "\n",
    "# 转换\n",
    "df = dataset_to_dataframe(ds)\n",
    "\n",
    "# 查看前几行数据\n",
    "df.head(2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 23:54:10.676746: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-05-04 23:55:44.227927: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                              date  polarity        query  \\\n",
       "0  b'Tue Jun 16 19:38:56 PDT 2009'         0  b'NO_QUERY'   \n",
       "1  b'Sun Jun 07 19:10:46 PDT 2009'         4  b'NO_QUERY'   \n",
       "\n",
       "                                       text             user  \n",
       "0  b'ouch! i just burned my finger so bad '  b'MISS_NIKKITA'  \n",
       "1          b\"@MCeeYOSHi that's my movie ! \"  b'imBIGtrouble'  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>polarity</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'Tue Jun 16 19:38:56 PDT 2009'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b'ouch! i just burned my finger so bad '</td>\n",
       "      <td>b'MISS_NIKKITA'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Sun Jun 07 19:10:46 PDT 2009'</td>\n",
       "      <td>4</td>\n",
       "      <td>b'NO_QUERY'</td>\n",
       "      <td>b\"@MCeeYOSHi that's my movie ! \"</td>\n",
       "      <td>b'imBIGtrouble'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.700087Z",
     "start_time": "2025-05-04T15:55:45.685222Z"
    }
   },
   "cell_type": "code",
   "source": "df.polarity.value_counts()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:45.758854Z",
     "start_time": "2025-05-04T15:55:45.741622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df['polarity'] = df.polarity.apply(lambda x: 0 if x == 0 else 1).to_numpy()\n",
    "df[\"polarity\"] = df[\"polarity\"].replace(4, 1) \n",
    "df.polarity.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:49.444343Z",
     "start_time": "2025-05-04T15:55:45.804598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "texts = df[\"text\"].values\n",
    "\n",
    "# 文本清洗函数\n",
    "def clean_text(text):\n",
    "    # 确保文本是字符串类型\n",
    "    text = text.decode('utf-8') if isinstance(text, bytes) else text\n",
    "    # 移除@用户名\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)          \n",
    "    # 移除URL\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  \n",
    "    # 只保留字母和空格\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)   \n",
    "    # 转为小写\n",
    "    text = text.lower().strip()                \n",
    "    return text\n",
    "\n",
    "# 应用清洗\n",
    "cleaned_texts = [clean_text(text) for text in texts]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:51.321232Z",
     "start_time": "2025-05-04T15:55:49.511497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = np.array(cleaned_texts)\n",
    "labels = df['polarity'].to_numpy()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences, labels))\n",
    "\n",
    "for i, (sentence, label) in enumerate(dataset.take(2)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Sentence: {sentence.numpy().decode('utf-8')}\")\n",
    "    print(f\"Label: {label.numpy()}\")\n",
    "    print(\"-\" * 50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Sentence: ouch i just burned my finger so bad\n",
      "Label: 0\n",
      "--------------------------------------------------\n",
      "Sample 2:\n",
      "Sentence: thats my movie\n",
      "Label: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 23:55:51.318501: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T15:55:51.366097Z",
     "start_time": "2025-05-04T15:55:51.353813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置训练集比例\n",
    "# 计算训练集大小\n",
    "total_size = len(sentences)\n",
    "train_size = int(total_size * TRAINING_SPLIT)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# 打乱数据集（使用 buffer_size 作为参数，推荐设置为数据集大小）\n",
    "dataset = dataset.shuffle(buffer_size=total_size, reshuffle_each_iteration=True)\n",
    "\n",
    "# 将数据集拆分为训练集和验证集\n",
    "train_dataset = dataset.take(train_size)\n",
    "validation_dataset = dataset.skip(train_size)\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "train_dataset = (train_dataset\n",
    "                   .shuffle(10000)\n",
    "                   .cache()\n",
    "                   .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                   .batch(BATCH_SIZE)\n",
    "                   )\n",
    "validation_dataset = (validation_dataset\n",
    "                  .cache()\n",
    "                  .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                  .batch(BATCH_SIZE)\n",
    "                  )\n",
    "\n",
    "print(f\"There are {len(train_dataset)} batches for a total of {BATCH_SIZE*len(train_dataset)} elements for training.\\n\")\n",
    "print(f\"There are {len(validation_dataset)} batches for a total of {BATCH_SIZE*len(validation_dataset)} elements for validation.\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11250 batches for a total of 1440000 elements for training.\n",
      "\n",
      "There are 1250 batches for a total of 160000 elements for validation.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NLP\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:06:06.819660Z",
     "start_time": "2025-05-04T16:06:06.816225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit_vectorizer(dataset):\n",
    "    vectorizer = tf.keras.layers.TextVectorization( \n",
    "        # max_tokens=10000, # 生成的矢量化词库的最大词数\n",
    "        output_sequence_length=MAX_LENGTH,\n",
    "        standardize='lower_and_strip_punctuation'\n",
    "    ) \n",
    "    full_tokens = dataset.map(lambda x, y: x)\n",
    "    vectorizer.adapt(full_tokens)\n",
    "    return vectorizer"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T16:27:12.564431Z",
     "start_time": "2025-05-04T16:06:09.085693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adapt the vectorizer to the training sentences\n",
    "vectorizer = fit_vectorizer(train_dataset)\n",
    "# Check size of vocabulary\n",
    "vocab_size = vectorizer.vocabulary_size()\n",
    "vocab_size"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 00:06:19.146912: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 459694 of 1600000\n",
      "2025-05-05 00:06:39.146241: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 483535 of 1600000\n",
      "2025-05-05 00:06:59.147111: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 509269 of 1600000\n",
      "2025-05-05 00:07:19.146350: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 539087 of 1600000\n",
      "2025-05-05 00:07:39.147164: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 557258 of 1600000\n",
      "2025-05-05 00:07:59.146013: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 573851 of 1600000\n",
      "2025-05-05 00:08:19.146470: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 592249 of 1600000\n",
      "2025-05-05 00:08:39.146798: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 612125 of 1600000\n",
      "2025-05-05 00:08:59.146747: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 628381 of 1600000\n",
      "2025-05-05 00:09:19.146460: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 648041 of 1600000\n",
      "2025-05-05 00:09:39.145996: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 674977 of 1600000\n",
      "2025-05-05 00:09:49.146163: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 682643 of 1600000\n",
      "2025-05-05 00:09:59.146219: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 691592 of 1600000\n",
      "2025-05-05 00:10:19.146156: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 707257 of 1600000\n",
      "2025-05-05 00:10:29.147174: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 714356 of 1600000\n",
      "2025-05-05 00:10:49.146386: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 730133 of 1600000\n",
      "2025-05-05 00:11:09.146781: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 745901 of 1600000\n",
      "2025-05-05 00:11:29.147187: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 761540 of 1600000\n",
      "2025-05-05 00:11:49.146187: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 777076 of 1600000\n",
      "2025-05-05 00:11:59.146297: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 783916 of 1600000\n",
      "2025-05-05 00:12:19.145960: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 805338 of 1600000\n",
      "2025-05-05 00:12:29.146054: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 815984 of 1600000\n",
      "2025-05-05 00:12:39.146859: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 823986 of 1600000\n",
      "2025-05-05 00:12:59.147260: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 839173 of 1600000\n",
      "2025-05-05 00:13:19.146181: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 854126 of 1600000\n",
      "2025-05-05 00:13:29.147140: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 863322 of 1600000\n",
      "2025-05-05 00:13:49.146351: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 879457 of 1600000\n",
      "2025-05-05 00:14:09.146545: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 894160 of 1600000\n",
      "2025-05-05 00:14:19.147248: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 903598 of 1600000\n",
      "2025-05-05 00:14:39.146041: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 921596 of 1600000\n",
      "2025-05-05 00:14:59.146931: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 935495 of 1600000\n",
      "2025-05-05 00:15:19.146971: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 951285 of 1600000\n",
      "2025-05-05 00:15:39.146852: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 967871 of 1600000\n",
      "2025-05-05 00:15:59.146401: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 982829 of 1600000\n",
      "2025-05-05 00:16:19.146624: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 998417 of 1600000\n",
      "2025-05-05 00:16:29.146776: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1005767 of 1600000\n",
      "2025-05-05 00:16:49.146840: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1021652 of 1600000\n",
      "2025-05-05 00:17:09.147363: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1037492 of 1600000\n",
      "2025-05-05 00:17:29.146227: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1053292 of 1600000\n",
      "2025-05-05 00:17:39.146871: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1067504 of 1600000\n",
      "2025-05-05 00:17:59.145890: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1082261 of 1600000\n",
      "2025-05-05 00:18:19.146047: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1097634 of 1600000\n",
      "2025-05-05 00:18:39.146646: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1115058 of 1600000\n",
      "2025-05-05 00:18:59.146022: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1130423 of 1600000\n",
      "2025-05-05 00:19:19.147416: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1146021 of 1600000\n",
      "2025-05-05 00:19:39.145969: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1161973 of 1600000\n",
      "2025-05-05 00:19:49.146971: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1170532 of 1600000\n",
      "2025-05-05 00:19:59.147168: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1177836 of 1600000\n",
      "2025-05-05 00:20:19.147190: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1193769 of 1600000\n",
      "2025-05-05 00:20:39.146173: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1210572 of 1600000\n",
      "2025-05-05 00:20:49.147325: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1217688 of 1600000\n",
      "2025-05-05 00:21:09.147642: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1231595 of 1600000\n",
      "2025-05-05 00:21:29.147168: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1247483 of 1600000\n",
      "2025-05-05 00:21:49.146735: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1263123 of 1600000\n",
      "2025-05-05 00:22:09.146377: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1278745 of 1600000\n",
      "2025-05-05 00:22:29.146172: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1294373 of 1600000\n",
      "2025-05-05 00:22:39.146593: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1300905 of 1600000\n",
      "2025-05-05 00:22:59.147133: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1316795 of 1600000\n",
      "2025-05-05 00:23:19.146665: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1332592 of 1600000\n",
      "2025-05-05 00:23:39.146658: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1349814 of 1600000\n",
      "2025-05-05 00:23:59.146702: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1365646 of 1600000\n",
      "2025-05-05 00:24:19.147151: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1381327 of 1600000\n",
      "2025-05-05 00:24:39.146816: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1398350 of 1600000\n",
      "2025-05-05 00:24:59.146137: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1415206 of 1600000\n",
      "2025-05-05 00:25:19.147086: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1431679 of 1600000\n",
      "2025-05-05 00:25:39.146585: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1447641 of 1600000\n",
      "2025-05-05 00:25:49.147360: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1454860 of 1600000\n",
      "2025-05-05 00:26:09.146217: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1470342 of 1600000\n",
      "2025-05-05 00:26:29.146770: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1486269 of 1600000\n",
      "2025-05-05 00:26:49.146168: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1502133 of 1600000\n",
      "2025-05-05 00:27:09.146642: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1517717 of 1600000\n",
      "2025-05-05 00:27:19.147009: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:24: Filling up shuffle buffer (this may take a while): 1524391 of 1600000\n",
      "2025-05-05 00:27:31.984058: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n",
      "2025-05-05 00:27:31.984138: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:27: Filling up shuffle buffer (this may take a while): 1 of 10000\n",
      "2025-05-05 00:27:32.086119: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Adapt the vectorizer to the training sentences\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m fit_vectorizer(train_dataset)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Check size of vocabulary\u001B[39;00m\n\u001B[1;32m      4\u001B[0m vocab_size \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mvocabulary_size()\n",
      "Cell \u001B[0;32mIn[11], line 8\u001B[0m, in \u001B[0;36mfit_vectorizer\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m      2\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mTextVectorization( \n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# max_tokens=10000, # 生成的矢量化词库的最大词数\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     output_sequence_length\u001B[38;5;241m=\u001B[39mMAX_LENGTH,\n\u001B[1;32m      5\u001B[0m     standardize\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlower_and_strip_punctuation\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m ) \n\u001B[1;32m      7\u001B[0m full_tokens \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: x)\n\u001B[0;32m----> 8\u001B[0m vectorizer\u001B[38;5;241m.\u001B[39madapt(full_tokens)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m vectorizer\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/colab/lib/python3.11/site-packages/keras/src/layers/preprocessing/text_vectorization.py:420\u001B[0m, in \u001B[0;36mTextVectorization.adapt\u001B[0;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    419\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mtake(steps)\n\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_state(batch)\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/colab/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    825\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_internal()\n\u001B[1;32m    827\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOutOfRangeError:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/colab/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    773\u001B[0m \u001B[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[1;32m    774\u001B[0m \u001B[38;5;66;03m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecution_mode(context\u001B[38;5;241m.\u001B[39mSYNC):\n\u001B[0;32m--> 776\u001B[0m   ret \u001B[38;5;241m=\u001B[39m gen_dataset_ops\u001B[38;5;241m.\u001B[39miterator_get_next(\n\u001B[1;32m    777\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_resource,\n\u001B[1;32m    778\u001B[0m       output_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_types,\n\u001B[1;32m    779\u001B[0m       output_shapes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_shapes)\n\u001B[1;32m    781\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_element_spec\u001B[38;5;241m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/colab/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3081\u001B[0m, in \u001B[0;36miterator_get_next\u001B[0;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[1;32m   3079\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   3080\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3081\u001B[0m     _result \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_FastPathExecute(\n\u001B[1;32m   3082\u001B[0m       _ctx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteratorGetNext\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, iterator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_types\u001B[39m\u001B[38;5;124m\"\u001B[39m, output_types,\n\u001B[1;32m   3083\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_shapes\u001B[39m\u001B[38;5;124m\"\u001B[39m, output_shapes)\n\u001B[1;32m   3084\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3085\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset_vectorized = train_dataset.map(lambda x,y: (vectorizer(x), y))\n",
    "validation_dataset_vectorized = validation_dataset.map(lambda x,y: (vectorizer(x), y))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "glove_file = '../data/glove.6B.100d.txt'\n",
    "\n",
    "# Initialize an empty embeddings index dictionary\n",
    "glove_embeddings = {}\n",
    "\n",
    "# Read file and fill glove_embeddings with its contents\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = coefs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "word_index = {x:i for i,x in enumerate(vectorizer.get_vocabulary())}"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "# Iterate all of the words in the vocabulary and if the vector representation for \n",
    "# each word exists within GloVe's representations, save it in the embeddings_matrix array\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 建模"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def create_model(vocab_size, pretrained_embeddings):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(None,)),\n",
    "        tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[pretrained_embeddings]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            16,  # 减少单元数\n",
    "            return_sequences=True,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3,\n",
    "            kernel_regularizer=regularizers.l2(0.01)\n",
    "        )),\n",
    "        tf.keras.layers.Dropout(0.6),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            8,  # 减少单元数\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3)\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.6),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = create_model(vocab_size, embeddings_matrix)\n",
    "history = model.fit(\n",
    "\ttrain_dataset_vectorized, \n",
    "\tepochs=20, \n",
    "\tvalidation_data=validation_dataset_vectorized\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle('Training and validation performance')\n",
    "\n",
    "for i, (data, label) in enumerate(zip([(acc, val_acc), (loss, val_loss)], [\"Accuracy\", \"Loss\"])):\n",
    "    ax[i].plot(epochs, data[0], 'r', label=\"Training \" + label)\n",
    "    ax[i].plot(epochs, data[1], 'b', label=\"Validation \" + label)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('epochs')"
   ]
  }
 ]
}
